"""
Combined Regularization: Dropout + L2 Together
===============================================

PREREQUISITES:
- Complete network_3.2.0.py (understand dropout)
- Complete network_3.2.1.py (understand dropout rates)
- Complete network_3.2.2.py (understand dropout vs L2)

THIS EXPERIMENT:
Combines BOTH dropout AND L2 regularization together to achieve the best
possible generalization. Demonstrates that different regularization techniques
complement each other for maximum performance!

THE POWER OF COMPLEMENTARY REGULARIZATION:

Why Combine Multiple Regularization Techniques?

Different Problems Require Different Solutions:
  â€¢ L2 prevents individual weights from becoming too large
  â€¢ Dropout prevents groups of neurons from co-adapting
  â€¢ Together they address different aspects of overfitting!

Complementary Mechanisms:
  â€¢ L2: Continuous, deterministic, affects all weights equally
  â€¢ Dropout: Discrete, stochastic, affects neural dependencies
  â€¢ Non-overlapping mechanisms â†’ additive benefits!

THE TWO TECHNIQUES IN DETAIL:

L2 Regularization (Î» = 0.1):
  Cost: C = Câ‚€ + (Î»/2n)Î£wÂ²
  
  What it does:
    â€¢ Penalizes large weight magnitudes
    â€¢ Shrinks weights toward zero
    â€¢ Simpler decision boundaries
    â€¢ Prevents extreme weight values
  
  What it DOESN'T do:
    â€¢ Doesn't prevent co-adaptation
    â€¢ Doesn't create ensemble effect
    â€¢ Doesn't force redundancy

Dropout (p = 0.5):
  During training: Randomly drop 50% of neurons
  
  What it does:
    â€¢ Prevents co-adaptation of neurons
    â€¢ Creates ensemble of 2^n networks
    â€¢ Forces redundant representations
    â€¢ Makes neurons independent
  
  What it DOESN'T do:
    â€¢ Doesn't directly limit weight magnitude
    â€¢ Doesn't guarantee small weights
    â€¢ Doesn't simplify individual neurons

Together (L2 + Dropout):
  BEST OF BOTH WORLDS!
  
  L2 ensures:
    âœ“ No individual weight becomes extreme
    âœ“ Overall weight magnitudes stay reasonable
    âœ“ Simpler base model
  
  Dropout ensures:
    âœ“ Neurons work independently
    âœ“ Ensemble effect for robustness
    âœ“ Redundant, reliable representations
  
  Result: Maximum generalization!

THE EXPERIMENT:

We compare four training configurations:

Configuration A: No Regularization
  Conv â†’ Conv â†’ FC â†’ Softmax
  Î»=0.0, dropout=0.0
  Expected: ~98.0% (severe overfitting)

Configuration B: L2 Only
  Conv â†’ Conv â†’ FC â†’ Softmax
  Î»=0.1, dropout=0.0
  Expected: ~98.7% (good)

Configuration C: Dropout Only
  Conv â†’ Conv â†’ FC (dropout 0.5) â†’ Softmax
  Î»=0.0, dropout=0.5
  Expected: ~98.9% (better)

Configuration D: L2 + Dropout (BOTH!)
  Conv â†’ Conv â†’ FC (dropout 0.5) â†’ Softmax
  Î»=0.1, dropout=0.5
  Expected: ~99.2% (BEST!)

Result: Combined regularization achieves highest accuracy!

Expected Results:
- No regularization: ~98.0% (overfitting)
- L2 only: ~98.7%
- Dropout only: ~98.9%
- L2 + Dropout: ~99.2% (BEST! ğŸ†)
- Key lesson: Complementary regularization techniques compound benefits

WHY THEY WORK TOGETHER:

Example: The 100-Neuron FC Layer

Without Any Regularization:
  â€¢ Neuron 1: weight = 5.7 (too large!)
  â€¢ Neuron 2: relies on neuron 1 (co-adaptation)
  â€¢ Neuron 3: weight = 8.2 (extreme!)
  â€¢ Neuron 4: relies on neurons 1,2,3 (fragile)
  â€¢ Result: Memorizes training data, doesn't generalize

With L2 Only:
  â€¢ Neuron 1: weight = 0.8 (shrunk by L2) âœ“
  â€¢ Neuron 2: relies on neuron 1 (still co-adapts) âœ—
  â€¢ Neuron 3: weight = 1.1 (shrunk) âœ“
  â€¢ Neuron 4: relies on 1,2,3 (still fragile) âœ—
  â€¢ Result: Weights smaller but dependencies remain

With Dropout Only:
  â€¢ Neuron 1: weight = 3.2 (not directly controlled) ~
  â€¢ Neuron 2: independent (dropout forced it) âœ“
  â€¢ Neuron 3: weight = 4.1 (can be large) ~
  â€¢ Neuron 4: independent (learned without 1,2,3) âœ“
  â€¢ Result: Independent but weights can be large

With L2 + Dropout:
  â€¢ Neuron 1: weight = 0.7 (L2 shrunk) âœ“
  â€¢ Neuron 2: independent (dropout forced) âœ“
  â€¢ Neuron 3: weight = 0.9 (L2 shrunk) âœ“
  â€¢ Neuron 4: independent (dropout forced) âœ“
  â€¢ Result: Small weights AND independent neurons! âœ“âœ“

This is why combining them works so well!

TUNING THE COMBINATION:

The hyperparameter space:
  â€¢ L2: Î» âˆˆ [0, 10]
  â€¢ Dropout: p âˆˆ [0, 0.8]

Safe defaults (work 90% of the time):
  â€¢ L2: Î» = 0.1
  â€¢ Dropout: p = 0.5

For larger networks:
  â€¢ L2: Î» = 0.01 to 0.1 (lighter)
  â€¢ Dropout: p = 0.5 to 0.6 (standard or slightly higher)

For smaller networks:
  â€¢ L2: Î» = 0.5 to 5.0 (heavier)
  â€¢ Dropout: p = 0.2 to 0.4 (lighter)

General principle:
  More regularization â‰  better!
  Need to balance with model capacity.

NEXT STEPS:
This completes the dropout series! Next:
- network_3.3.x: Optimized CNN architectures for 99.5%+ accuracy

Run: python network_3.2.3.py
"""

import sys
sys.path.append('../src')
import network3
from network3 import Network, ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer, ReLU

def main():
    # ========================================================================
    # EXPERIMENT: Combined L2 + Dropout Regularization
    # ========================================================================
    print("=" * 75)
    print("COMBINED REGULARIZATION: L2 + Dropout Together")
    print("=" * 75)

    # Load MNIST data
    training_data, validation_data, test_data = network3.load_data_shared()
    mini_batch_size = 10

    # ========================================================================
    # CONFIGURATION A: No Regularization (Baseline)
    # ========================================================================
    # PURPOSE: Show the overfitting problem clearly
    #
    # REGULARIZATION:
    #   â€¢ L2: Î» = 0.0 (no weight decay)
    #   â€¢ Dropout: p = 0.0 (no dropout)
    #
    # EXPECTED:
    #   â€¢ Training: ~100% (memorizes training data)
    #   â€¢ Validation: ~98.0%
    #   â€¢ Gap: ~2% (severe overfitting)
    #   â€¢ Problem: Too much capacity, no constraints
    
    print("\n" + "=" * 75)
    print("[CONFIG A: No Regularization - Baseline]")
    print("=" * 75)
    
    layer1_none = ConvPoolLayer(
        filter_shape=(20, 1, 5, 5),
        image_shape=(mini_batch_size, 1, 28, 28),
        poolsize=(2, 2),
        activation_fn=ReLU
    )
    
    layer2_none = ConvPoolLayer(
        filter_shape=(40, 20, 5, 5),
        image_shape=(mini_batch_size, 20, 12, 12),
        poolsize=(2, 2),
        activation_fn=ReLU
    )
    
    layer3_none = FullyConnectedLayer(
        n_in=40*4*4,
        n_out=100,
        activation_fn=ReLU,
        p_dropout=0.0                            # NO DROPOUT
    )
    
    layer4_none = SoftmaxLayer(n_in=100, n_out=10, p_dropout=0.0)
    
    net_none = Network([layer1_none, layer2_none, layer3_none, layer4_none], 
                       mini_batch_size)
    
    print("\nTraining baseline (NO regularization)...")
    
    net_none.SGD(training_data, 30, mini_batch_size, 0.03,
                validation_data, test_data, lmbda=0.0)

    # ========================================================================
    # CONFIGURATION B: L2 Only
    # ========================================================================
    # L2 REGULARIZATION:
    #   â€¢ Î» = 0.1 (standard value)
    #   â€¢ Adds (Î»/2n)Î£wÂ² to cost
    #   â€¢ Shrinks all weights toward zero
    #   â€¢ Prevents extreme weight values
    #
    # EXPECTED:
    #   â€¢ Training: ~99%
    #   â€¢ Validation: ~98.7%
    #   â€¢ Gap: ~0.3%
    #   â€¢ Improvement: +0.7% over baseline
    #   â€¢ Mechanism: Prevents large weights
    
    print("\n" + "=" * 75)
    print("[CONFIG B: L2 Regularization Only]")
    print("=" * 75)
    
    layer1_l2 = ConvPoolLayer(
        filter_shape=(20, 1, 5, 5),
        image_shape=(mini_batch_size, 1, 28, 28),
        poolsize=(2, 2),
        activation_fn=ReLU
    )
    
    layer2_l2 = ConvPoolLayer(
        filter_shape=(40, 20, 5, 5),
        image_shape=(mini_batch_size, 20, 12, 12),
        poolsize=(2, 2),
        activation_fn=ReLU
    )
    
    layer3_l2 = FullyConnectedLayer(
        n_in=40*4*4,
        n_out=100,
        activation_fn=ReLU,
        p_dropout=0.0                            # NO DROPOUT
    )
    
    layer4_l2 = SoftmaxLayer(n_in=100, n_out=10, p_dropout=0.0)
    
    net_l2 = Network([layer1_l2, layer2_l2, layer3_l2, layer4_l2],
                    mini_batch_size)
    
    print("\nTraining with L2 only (Î»=0.1)...")
    
    net_l2.SGD(training_data, 30, mini_batch_size, 0.03,
              validation_data, test_data, lmbda=0.1)      # L2 only

    # ========================================================================
    # CONFIGURATION C: Dropout Only
    # ========================================================================
    # DROPOUT REGULARIZATION:
    #   â€¢ p = 0.5 (standard dropout rate)
    #   â€¢ Randomly drops 50% of FC neurons
    #   â€¢ Prevents co-adaptation
    #   â€¢ Creates ensemble effect
    #
    # EXPECTED:
    #   â€¢ Training: ~99%
    #   â€¢ Validation: ~98.9%
    #   â€¢ Gap: ~0.1%
    #   â€¢ Improvement: +0.9% over baseline
    #   â€¢ Mechanism: Prevents co-adaptation + ensemble
    
    print("\n" + "=" * 75)
    print("[CONFIG C: Dropout Only]")
    print("=" * 75)
    
    layer1_dropout = ConvPoolLayer(
        filter_shape=(20, 1, 5, 5),
        image_shape=(mini_batch_size, 1, 28, 28),
        poolsize=(2, 2),
        activation_fn=ReLU
    )
    
    layer2_dropout = ConvPoolLayer(
        filter_shape=(40, 20, 5, 5),
        image_shape=(mini_batch_size, 20, 12, 12),
        poolsize=(2, 2),
        activation_fn=ReLU
    )
    
    layer3_dropout = FullyConnectedLayer(
        n_in=40*4*4,
        n_out=100,
        activation_fn=ReLU,
        p_dropout=0.5                            # DROPOUT!
    )
    
    layer4_dropout = SoftmaxLayer(n_in=100, n_out=10, p_dropout=0.0)
    
    net_dropout = Network([layer1_dropout, layer2_dropout, layer3_dropout, layer4_dropout],
                         mini_batch_size)
    
    print("\nTraining with dropout only (p=0.5)...")
    
    net_dropout.SGD(training_data, 30, mini_batch_size, 0.03,
                   validation_data, test_data, lmbda=0.0)  # No L2

    # ========================================================================
    # CONFIGURATION D: L2 + Dropout COMBINED! ğŸ†
    # ========================================================================
    # COMBINED REGULARIZATION:
    #   â€¢ L2: Î» = 0.1 (prevents large weights)
    #   â€¢ Dropout: p = 0.5 (prevents co-adaptation)
    #   â€¢ Both mechanisms work together!
    #
    # HOW THEY COMPLEMENT:
    #   L2: Keeps weights small â†’ simpler base model
    #   Dropout: Keeps neurons independent â†’ robust features
    #   Together: Simple AND robust â†’ best generalization!
    #
    # EXPECTED:
    #   â€¢ Training: ~99%
    #   â€¢ Validation: ~99.2%
    #   â€¢ Gap: ~0% (no overfitting!)
    #   â€¢ Improvement: +1.2% over baseline
    #   â€¢ Mechanism: Both L2 AND dropout working together
    #
    # WHY THIS WORKS BEST:
    #   â€¢ L2 constrains individual weights
    #   â€¢ Dropout constrains neural dependencies
    #   â€¢ Different aspects of the same problem
    #   â€¢ Non-overlapping mechanisms â†’ additive benefits!
    
    print("\n" + "=" * 75)
    print("[CONFIG D: L2 + Dropout COMBINED! ğŸ†]")
    print("=" * 75)
    
    layer1_combined = ConvPoolLayer(
        filter_shape=(20, 1, 5, 5),
        image_shape=(mini_batch_size, 1, 28, 28),
        poolsize=(2, 2),
        activation_fn=ReLU
    )
    
    layer2_combined = ConvPoolLayer(
        filter_shape=(40, 20, 5, 5),
        image_shape=(mini_batch_size, 20, 12, 12),
        poolsize=(2, 2),
        activation_fn=ReLU
    )
    
    layer3_combined = FullyConnectedLayer(
        n_in=40*4*4,
        n_out=100,
        activation_fn=ReLU,
        p_dropout=0.5                            # DROPOUT!
    )
    
    layer4_combined = SoftmaxLayer(n_in=100, n_out=10, p_dropout=0.0)
    
    net_combined = Network([layer1_combined, layer2_combined, layer3_combined, layer4_combined],
                          mini_batch_size)
    
    print("\nTraining with BOTH L2 + Dropout (Î»=0.1, p=0.5)...")
    
    net_combined.SGD(training_data, 40, mini_batch_size, 0.03,
                    validation_data, test_data, lmbda=0.1)  # BOTH!

    # ========================================================================
    # Comprehensive Results Summary and Analysis
    # ========================================================================
    # EXPECTED FINAL RESULTS:
    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚ Configuration     â”‚ Train Acc   â”‚ Val Accuracy â”‚ Gap     â”‚ Improvement  â”‚ Rank       â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚ A: None           â”‚   ~100%     â”‚    ~98.0%    â”‚ ~2.0%   â”‚   baseline   â”‚ 4th (worst)â”‚
    # â”‚ B: L2 only        â”‚   ~99%      â”‚    ~98.7%    â”‚ ~0.3%   â”‚   +0.7%      â”‚ 3rd        â”‚
    # â”‚ C: Dropout only   â”‚   ~99%      â”‚    ~98.9%    â”‚ ~0.1%   â”‚   +0.9%      â”‚ 2nd        â”‚
    # â”‚ D: L2 + Dropout   â”‚   ~99%      â”‚    ~99.2%    â”‚ ~0.0%   â”‚   +1.2% ğŸ†   â”‚ 1st (BEST!)â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    #
    # PERFORMANCE ANALYSIS:
    #
    # Absolute Accuracy:
    #   â€¢ Baseline (no reg):     98.0%
    #   â€¢ L2 only:               98.7%  (+0.7%)
    #   â€¢ Dropout only:          98.9%  (+0.9%)
    #   â€¢ L2 + Dropout:          99.2%  (+1.2%) â­ BEST
    #
    # Error Rates (per 10,000 test images):
    #   â€¢ Baseline:              200 errors
    #   â€¢ L2 only:               130 errors  (35% error reduction)
    #   â€¢ Dropout only:          110 errors  (45% error reduction)
    #   â€¢ L2 + Dropout:           80 errors  (60% error reduction!) ğŸ¯
    #
    # Improvement Breakdown:
    #   â€¢ Adding L2 to baseline:           +0.7%
    #   â€¢ Adding dropout to baseline:      +0.9%
    #   â€¢ Adding BOTH to baseline:         +1.2%
    #   
    #   Key insight: 0.7 + 0.9 â‰ˆ 1.2 (mostly complementary!)
    #
    # WHY COMBINED REGULARIZATION WINS:
    #
    # 1. COMPLEMENTARY MECHANISMS
    #    
    #    L2 Regularization:
    #      â€¢ Constraint: Î£wÂ² must stay small
    #      â€¢ Effect: Individual weights shrink
    #      â€¢ Solves: Weight magnitude problem
    #    
    #    Dropout:
    #      â€¢ Constraint: Neurons must work independently
    #      â€¢ Effect: Forces redundancy and robustness
    #      â€¢ Solves: Co-adaptation problem
    #    
    #    Together:
    #      â€¢ L2: Small weights â†’ simpler neurons
    #      â€¢ Dropout: Independent neurons â†’ robust features
    #      â€¢ Different problems, different solutions!
    #      â€¢ Results compound!
    #
    # 2. DOUBLE DEFENSE AGAINST OVERFITTING
    #    
    #    Overfitting can happen through:
    #    
    #    Path 1: Large weights memorize data
    #      Defense: L2 regularization âœ“
    #    
    #    Path 2: Co-adapted neurons memorize patterns
    #      Defense: Dropout âœ“
    #    
    #    Path 3: Large weights AND co-adaptation (worst case!)
    #      Defense: L2 + Dropout âœ“âœ“
    #    
    #    Combined regularization blocks BOTH paths!
    #
    # 3. TRAINING DYNAMICS COMPARISON
    #    
    #    No Regularization:
    #      â€¢ Learns useful features â†’ starts memorizing â†’ severe overfitting
    #      â€¢ Result: High train, lower validation
    #    
    #    L2 Only:
    #      â€¢ Weights shrink throughout training
    #      â€¢ Prevents extreme memorization
    #      â€¢ But neurons can still co-adapt
    #    
    #    Dropout Only:
    #      â€¢ Neurons forced to be independent
    #      â€¢ Creates robust features
    #      â€¢ But individual weights can be large
    #    
    #    L2 + Dropout:
    #      â€¢ Weights shrink (L2) AND neurons independent (dropout)
    #      â€¢ Can't memorize through large weights
    #      â€¢ Can't memorize through co-adaptation
    #      â€¢ Both overfitting paths blocked! â†’ BEST performance
    #
    # 4. WHEN COMBINED REGULARIZATION MATTERS MOST
    #    
    #    Small Networks (<10K parameters):
    #      â€¢ L2 alone often sufficient
    #      â€¢ Combined: ~0.1-0.2% improvement
    #    
    #    Medium Networks (10K-100K parameters):
    #      â€¢ Both L2 and dropout important
    #      â€¢ Combined: ~0.3-0.5% improvement âœ“
    #      â€¢ THIS EXPERIMENT falls here!
    #    
    #    Large Networks (100K-1M parameters):
    #      â€¢ Dropout critical, L2 helps
    #      â€¢ Combined: ~0.5-1.0% improvement âœ“âœ“
    #    
    #    Very Large Networks (>1M parameters):
    #      â€¢ Both essential
    #      â€¢ Combined: ~1.0-2.0% improvement âœ“âœ“âœ“
    #
    # 5. PRACTICAL GUIDELINES
    #    
    #    Default Recipe (use this 90% of the time):
    #      âœ“ Dropout: p=0.5 on all FC layers
    #      âœ“ L2: Î»=0.1 on all weights
    #      âœ“ No dropout on conv layers (optional: p=0.1-0.2)
    #      âœ“ No dropout on output layer (p=0)
    #    
    #    For Smaller Networks:
    #      â€¢ Reduce dropout: p=0.3-0.4
    #      â€¢ Increase L2: Î»=0.5-1.0
    #    
    #    For Larger Networks:
    #      â€¢ Keep dropout: p=0.5
    #      â€¢ Reduce L2: Î»=0.01-0.1
    #    
    #    Signs You Need More Regularization:
    #      â€¢ Train accuracy >> validation accuracy
    #      â€¢ Gap increasing with more epochs
    #    
    #    Signs You Have Too Much Regularization:
    #      â€¢ Both train and validation accuracy low
    #      â€¢ Slow learning, plateaus early
    #
    # 6. REAL-WORLD EVIDENCE
    #    
    #    AlexNet (2012):
    #      â€¢ Used: Dropout (p=0.5) + Weight decay (L2)
    #      â€¢ Result: Won ImageNet (16.4% error)
    #      â€¢ Without combined reg: ~25% error
    #      â€¢ Impact: Enabled deep learning revolution
    #    
    #    VGGNet (2014):
    #      â€¢ Used: Dropout (p=0.5) + L2 (Î»=5e-4)
    #      â€¢ Result: 7.3% ImageNet error
    #      â€¢ Standard pattern adopted industry-wide
    #    
    #    Modern Architectures:
    #      â€¢ Almost all use multiple regularization
    #      â€¢ Dropout + L2 + batch norm + data augmentation
    #      â€¢ Layered defense against overfitting
    #
    # 7. THE PATH FROM 95% TO 99%+
    #    
    #    Chapter 1 (Basic Network):
    #      â€¢ No regularization â†’ ~95% accuracy
    #    
    #    Chapter 2 (+ L2):
    #      â€¢ Added L2 regularization â†’ ~97% accuracy (+2%)
    #    
    #    Chapter 3 Part 1 (+ CNNs):
    #      â€¢ Added convolutional layers â†’ ~98.5% accuracy (+1.5%)
    #    
    #    Chapter 3 Part 2 (+ Dropout):
    #      â€¢ Added dropout regularization â†’ ~98.9% accuracy (+0.4%)
    #    
    #    Chapter 3 Part 3 (+ Combined):
    #      â€¢ Combined L2 + Dropout â†’ ~99.2% accuracy (+0.3%)
    #      â€¢ Optimal regularization! ğŸ¯
    #    
    #    Next (Chapter 3 Part 4):
    #      â€¢ Optimized architectures â†’ ~99.5%+ accuracy
    #      â€¢ State-of-the-art!
    #
    # WHAT YOU'VE LEARNED:
    #   âœ“ How to combine multiple regularization techniques
    #   âœ“ Why L2 and dropout complement each other
    #   âœ“ That different techniques solve different problems
    #   âœ“ How to achieve ~99% accuracy on MNIST
    #   âœ“ Practical hyperparameter values (Î»=0.1, p=0.5)
    #   âœ“ When combined regularization matters most
    #   âœ“ That modern deep learning uses layered regularization
    #
    # PRACTICAL TAKEAWAY:
    #   "Always use dropout (p=0.5) + L2 (Î»=0.1) together
    #    for fully connected layers in deep networks!"
    #   
    #   This simple recipe works incredibly well and is
    #   used in almost all modern architectures.
    #
    # THE JOURNEY SO FAR:
    #   95% â†’ 97% â†’ 98.5% â†’ 98.9% â†’ 99.2%
    #   Each improvement compounds!
    #   Modern deep learning = many good techniques together!
    
    print("\n" + "=" * 75)
    print("COMPREHENSIVE RESULTS SUMMARY")
    print("=" * 75)
    print("Next: network_3.3.x for optimized architectures (99.5% + accuracy)")

if __name__ == "__main__":
    main()

